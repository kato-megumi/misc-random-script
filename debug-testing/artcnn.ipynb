{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "import re\n",
    "import math\n",
    "\n",
    "def conv_layer(in_channels, out_channels, kernel_size):\n",
    "    padding = int((kernel_size - 1) / 2)\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding)\n",
    "\n",
    "class artCNN(nn.Module):\n",
    "    def __init__(self, block_depth=4, num_feat=32):\n",
    "        super(artCNN, self).__init__()\n",
    "        self.act = nn.ReLU()\n",
    "        self.ps = nn.PixelShuffle(2)\n",
    "        io_channels = 3\n",
    "        self.conv_head = conv_layer(io_channels, num_feat, kernel_size=3)\n",
    "        self.conv_mid = nn.ModuleList(\n",
    "            [\n",
    "                conv_layer(num_feat, num_feat, kernel_size=3)\n",
    "                for _ in range(block_depth)\n",
    "            ]\n",
    "        )\n",
    "        self.conv_mid2 = conv_layer(num_feat, num_feat, kernel_size=3)\n",
    "        self.conv_tail = conv_layer(num_feat, io_channels*2**2, kernel_size=3)\n",
    "    def forward(self, x):\n",
    "        out = self.conv_head(x)\n",
    "        store = out\n",
    "        for conv in self.conv_mid:\n",
    "            out = self.act(conv(out))\n",
    "        out = self.conv_mid2(out)\n",
    "        out = store+out\n",
    "        out = self.conv_tail(out)\n",
    "        out = self.ps(out)\n",
    "        return torch.clamp(out, max=1.0, min=0.0)\n",
    "    \n",
    "    def import_param(self, filename):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "        with open(filename) as f:\n",
    "            text = f.read()\n",
    "        pattern = r'-?\\d+(\\.\\d{2,})(e-?\\d+)?'\n",
    "        iter = re.finditer(pattern, text)\n",
    "        convert(self.conv_head, iter)\n",
    "        for conv in self.conv_mid:\n",
    "            convert(conv, iter)\n",
    "        convert(self.conv_mid2, iter)\n",
    "        convert(self.conv_tail, iter, False)\n",
    "        check = next(iter, None)\n",
    "        if check == None:\n",
    "            print(\"pass\")\n",
    "        else:\n",
    "            print(\"---failed---\\n\", check)\n",
    "            \n",
    "def convert(c, iter, doswap=False):\n",
    "    swap = [0,2,1,3]\n",
    "    out_chan, in_chan, width, height = c.weight.shape\n",
    "    for to in range(math.ceil(out_chan/4)):\n",
    "        for ti in range(math.ceil(in_chan/4)):\n",
    "            for w in range(width):\n",
    "                for h in range(height):\n",
    "                    for i in range(min(4, in_chan)):\n",
    "                        for o in range(min(4, out_chan)):\n",
    "                            o = swap[o] if doswap else o\n",
    "                            c.weight.data[to*4+o, ti*4+i, h, w] = float(next(iter).group(0))\n",
    "        for o in range(min(4, out_chan)):\n",
    "            o = swap[o] if doswap else o\n",
    "            c.bias.data[to*4+o] = float(next(iter).group(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pil = torchvision.transforms.ToPILImage()\n",
    "to_tensor = torchvision.transforms.ToTensor()      \n",
    "            \n",
    "device = torch.device(\"cuda\")\n",
    "model = artCNN().to(device).half()\n",
    "model.import_param(\"R:/ArtCNN_C4F32_RGB.glsl\")\n",
    "model.eval()\n",
    "image = to_tensor(Image.open(\"C:/Users/khoi/Videos/koiama.png\").convert(\"RGB\")).unsqueeze(0).half().to(device)\n",
    "out = model(image)[0]\n",
    "display(to_pil(out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pass\n"
     ]
    }
   ],
   "source": [
    "model = artCNN(num_feat=32).to(device).half()\n",
    "model.import_param(\"R:/ArtCNN_C4F32_SH_RGB.glsl\")\n",
    "dummy_input = torch.randn(1, 3, 720, 1280).to(device).half()\n",
    "onnx_path = \"R:/ArtCNN_C4F32_SH_RGB.onnx\"\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    onnx_path,\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\n",
    "        \"input\": {0: \"batch_size\", 2: \"width\", 3: \"height\"},\n",
    "        \"output\": {0: \"batch_size\", 2: \"width\", 3: \"height\"},\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
