{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "import math\n",
    "\n",
    "import torchvision\n",
    "from IPython.display import Image, display\n",
    "from PIL import Image\n",
    "\n",
    "# import onnx\n",
    "\n",
    "to_pil = torchvision.transforms.ToPILImage()\n",
    "to_tensor = torchvision.transforms.ToTensor()\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "class CReLU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CReLU, self).__init__()\n",
    "    def forward(self, x):\n",
    "        # return torch.cat((F.relu(x), F.relu(-x)), 1)\n",
    "        return F.relu(torch.cat((x, -x), 1))\n",
    "\n",
    "def convert(c, iter, doswap=False):\n",
    "    swap = [0,2,1,3]\n",
    "    out_chan, in_chan, width, height = c.weight.shape\n",
    "    for to in range(math.ceil(out_chan/4)):\n",
    "        for ti in range(math.ceil(in_chan/4)):\n",
    "            for w in range(width):\n",
    "                for h in range(height):\n",
    "                    for i in range(min(4, in_chan)):\n",
    "                        for o in range(min(4, out_chan)):\n",
    "                            o = swap[o] if doswap else o\n",
    "                            c.weight.data[to*4+o, ti*4+i, w, h] = float(next(iter).group(0))\n",
    "        for o in range(min(4, out_chan)):\n",
    "            o = swap[o] if doswap else o\n",
    "            c.bias.data[to*4+o] = float(next(iter).group(0))\n",
    "            \n",
    "def debug(*args, **kwargs):\n",
    "    if False:\n",
    "        print(*args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Anime4kRestore(nn.Module):\n",
    "    def __init__(self, block_depth=8, block_stack=5, channel=12):\n",
    "        super(Anime4kRestore, self).__init__()\n",
    "        self.conv_head = nn.Conv2d(3, channel, kernel_size=3, padding=1)\n",
    "        self.conv_mid = nn.ModuleList([nn.Conv2d(\n",
    "            channel*2, channel, kernel_size=3, padding=1) for _ in range(block_depth-1)])\n",
    "        if block_stack != 1:\n",
    "            self.conv_tail = nn.Conv2d(\n",
    "                2*channel*block_stack, 3, kernel_size=1, padding=0)\n",
    "        else:\n",
    "            self.conv_tail = nn.Conv2d(2*channel, 3, kernel_size=3, padding=1)\n",
    "        self.crelu = CReLU()\n",
    "        self.block_no_stack = block_depth - block_stack\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.crelu(self.conv_head(x))\n",
    "        debug(self.conv_head.weight.shape)\n",
    "        if self.block_no_stack == 0:\n",
    "            depth_list = [out]\n",
    "            debug(0, \"----\")\n",
    "        else:\n",
    "            depth_list = []\n",
    "            debug(0, \"a\")\n",
    "        for i, conv in enumerate(self.conv_mid):\n",
    "            out = self.crelu(conv(out))\n",
    "            debug(conv.weight.shape)\n",
    "            if i >= self.block_no_stack - 1:\n",
    "                depth_list.append(out)\n",
    "                debug(i+1, \"----\")\n",
    "            else:\n",
    "                debug(i+1, \"a\")\n",
    "        out = self.conv_tail(torch.cat(depth_list, 1))\n",
    "        debug(self.conv_tail.weight.shape)\n",
    "        debug(\"out\")\n",
    "        # return torch.clamp(out + x, max=1.0, min=0.0)\n",
    "        return out + x\n",
    "\n",
    "    def import_param(self, filename):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "        with open(filename) as f:\n",
    "            text = f.read()\n",
    "        pattern = r'-?\\d+(\\.\\d{2,})(e-?\\d+)?'\n",
    "        iter = re.finditer(pattern, text)\n",
    "        convert(self.conv_head, iter)\n",
    "        for conv in self.conv_mid:\n",
    "            convert(conv, iter)\n",
    "        convert(self.conv_tail, iter)\n",
    "        check = next(iter, None)\n",
    "        if check == None:\n",
    "            debug(\"pass\")\n",
    "        else:\n",
    "            debug(\"---failed---\\n\", check)\n",
    "\n",
    "# image2 = Image.open(\n",
    "#     \"C://Users/khoi/Videos/Screenshot 2023-08-21 14-04-25.png\").convert(\"RGB\")\n",
    "# image2 = to_tensor(image2).unsqueeze(0).to(device)\n",
    "# out = model(image2)[0]\n",
    "# display(to_pil(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Anime4kUpscale(nn.Module):\n",
    "    def __init__(self, block_depth=8, block_stack=5, channel=12):\n",
    "        super(Anime4kUpscale, self).__init__()\n",
    "        self.conv_head = nn.Conv2d(3, channel, kernel_size=3, padding=1)\n",
    "        self.conv_mid = nn.ModuleList([nn.Conv2d(\n",
    "            channel*2, channel, kernel_size=3, padding=1) for _ in range(block_depth-1)])\n",
    "        if block_stack != 1:\n",
    "            self.conv_tail = nn.Conv2d(\n",
    "                2*channel*block_stack, 12, kernel_size=1, padding=0)\n",
    "        else:\n",
    "            self.conv_tail = nn.Conv2d(2*channel, 12, kernel_size=3, padding=1)\n",
    "        self.crelu = CReLU()\n",
    "        self.block_no_stack = block_depth - block_stack\n",
    "        self.ps = nn.PixelShuffle(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.crelu(self.conv_head(x))\n",
    "        debug(self.conv_head.weight.shape)\n",
    "        if self.block_no_stack == 0:\n",
    "            depth_list = [out]\n",
    "            debug(0, \"----\")\n",
    "        else:\n",
    "            depth_list = []\n",
    "            debug(0, \"a\")\n",
    "        for i, conv in enumerate(self.conv_mid):\n",
    "            out = self.crelu(conv(out))\n",
    "            debug(conv.weight.shape)\n",
    "            if i >= self.block_no_stack - 1:\n",
    "                depth_list.append(out)\n",
    "                debug(i+1, \"----\")\n",
    "            else:\n",
    "                debug(i+1, \"a\")\n",
    "        out = self.conv_tail(torch.cat(depth_list, 1))\n",
    "        debug(self.conv_tail.weight.shape)\n",
    "        debug(\"out\")\n",
    "        out = self.ps(out) + F.interpolate(x, scale_factor=2, mode='bilinear')\n",
    "        # return torch.clamp(out, max=1.0, min=0.0)\n",
    "        return out\n",
    "\n",
    "    def import_param(self, filename):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "        with open(filename) as f:\n",
    "            text = f.read()\n",
    "        pattern = r'-?\\d+(\\.\\d{4,})(e-?\\d+)?'\n",
    "        iter = re.finditer(pattern, text)\n",
    "        convert(self.conv_head, iter)\n",
    "        for conv in self.conv_mid:\n",
    "            convert(conv, iter)\n",
    "        convert(self.conv_tail, iter, True)\n",
    "        check = next(iter, None)\n",
    "        if check == None:\n",
    "            debug(\"pass\")\n",
    "        else:\n",
    "            debug(\"---failed---\\n\", check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(1, 3, 720, 1280).to(device).half()\n",
    "\n",
    "upscaleModel = [\n",
    "    (\"tmp/Anime4K_Upscale_CNN_x2_L.glsl\", \"Upscale_L\", 3, 1, 8),\n",
    "    (\"tmp/Anime4K_Upscale_CNN_x2_VL.glsl\", \"Upscale_VL\", 7, 7, 8),\n",
    "    (\"tmp/Anime4K_Upscale_CNN_x2_UL.glsl\", \"Upscale_UL\", 7, 5, 12),\n",
    "    (\"tmp/Anime4K_Upscale_Denoise_CNN_x2_L.glsl\", \"Upscale_Denoise_L\", 3, 1, 8),\n",
    "    (\"tmp/Anime4K_Upscale_Denoise_CNN_x2_VL.glsl\", \"Upscale_Denoise_VL\", 7, 7, 8),\n",
    "    (\"tmp/Anime4K_Upscale_Denoise_CNN_x2_UL.glsl\", \"Upscale_Denoise_UL\", 7, 5, 12),\n",
    "]\n",
    "restoreModel = [\n",
    "    (\"tmp/Anime4K_Restore_CNN_S.glsl\", \"Restore_S\", 3, 1, 4),\n",
    "    (\"tmp/Anime4K_Restore_CNN_M.glsl\", \"Restore_M\", 7, 7, 4),\n",
    "    (\"tmp/Anime4K_Restore_CNN_L.glsl\", \"Restore_L\", 4, 1, 8),\n",
    "    (\"tmp/Anime4K_Restore_CNN_VL.glsl\", \"Restore_VL\", 8, 7, 8),\n",
    "    (\"tmp/Anime4K_Restore_CNN_UL.glsl\", \"Restore_UL\", 8, 5, 12),\n",
    "    (\"tmp/Anime4K_Restore_CNN_Soft_S.glsl\", \"Restore_Soft_S\", 3, 1, 4),\n",
    "    (\"tmp/Anime4K_Restore_CNN_Soft_M.glsl\", \"Restore_Soft_M\", 7, 7, 4),\n",
    "    (\"tmp/Anime4K_Restore_CNN_Soft_L.glsl\", \"Restore_Soft_L\", 4, 1, 8),\n",
    "    (\"tmp/Anime4K_Restore_CNN_Soft_VL.glsl\", \"Restore_Soft_VL\", 8, 7, 8),\n",
    "    (\"tmp/Anime4K_Restore_CNN_Soft_UL.glsl\", \"Restore_Soft_UL\", 8, 5, 12),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for filename, name, a, b, c in upscaleModel:\n",
    "    onnx_path = f\"onnxModel/{name}.onnx\"\n",
    "    model = Anime4kUpscale(a, b, c).to(device).half()\n",
    "    model.import_param(filename)\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        dummy_input,\n",
    "        onnx_path,\n",
    "        input_names=[\"input\"],\n",
    "        output_names=[\"output\"],\n",
    "        dynamic_axes={\n",
    "            \"input\": {0: \"batch_size\", 2: \"width\", 3: \"height\"},\n",
    "            \"output\": {0: \"batch_size\", 2: \"width\", 3: \"height\"},\n",
    "        },\n",
    "    )\n",
    "    # torch.onnx.export(model, dummy_input, onnx_path, verbose=True)\n",
    "\n",
    "for filename, name, a, b, c in restoreModel:\n",
    "    onnx_path = f\"onnxModel/{name}.onnx\"\n",
    "    model = Anime4kRestore(a, b, c).to(device).half()\n",
    "    model.import_param(filename)\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        dummy_input,\n",
    "        onnx_path,\n",
    "        input_names=[\"input\"],\n",
    "        output_names=[\"output\"],\n",
    "        dynamic_axes={\n",
    "            \"input\": {0: \"batch_size\", 2: \"height\", 3: \"width\"},\n",
    "            \"output\": {0: \"batch_size\", 2: \"height\", 3: \"width\"},\n",
    "        },\n",
    "        opset_version=10,\n",
    "    )\n",
    "    # torch.onnx.export(model, dummy_input, onnx_path, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Anime4kUpscale2(nn.Module):\n",
    "    def __init__(self, block_depth=7, block_stack=5, channel=12):\n",
    "        super(Anime4kUpscale2, self).__init__()\n",
    "        self.conv_head = nn.Conv2d(3, channel, kernel_size=3, padding=1)\n",
    "        self.conv_mid = nn.ModuleList([nn.Conv2d(\n",
    "            channel, channel, kernel_size=3, padding=1) for _ in range(block_depth-1)])\n",
    "        if block_stack != 1:\n",
    "            self.conv_tail = nn.Conv2d(\n",
    "                channel*block_stack, 12, kernel_size=1, padding=0)\n",
    "        else:\n",
    "            self.conv_tail = nn.Conv2d(channel, 12, kernel_size=3, padding=1)\n",
    "        self.prelu = nn.ModuleList([nn.PReLU() for i in range(block_depth)])\n",
    "        self.block_no_stack = block_depth - block_stack\n",
    "        self.ps = nn.PixelShuffle(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.prelu[0](self.conv_head(x))\n",
    "        print(self.conv_head.weight.shape)\n",
    "        if self.block_no_stack == 0:\n",
    "            depth_list = [out]\n",
    "        else:\n",
    "            depth_list = []\n",
    "        for i, conv in enumerate(self.conv_mid):\n",
    "            out = self.prelu[i+1](conv(out))\n",
    "            print(conv.weight.shape)\n",
    "            if i >= self.block_no_stack - 1:\n",
    "                depth_list.append(out)\n",
    "        out = self.conv_tail(torch.cat(depth_list, 1))\n",
    "        out = self.ps(out) + F.interpolate(x, scale_factor=2, mode='bilinear')\n",
    "        # return torch.clamp(out, max=1.0, min=0.0)\n",
    "        return out\n",
    "    \n",
    "device = torch.device(\"cuda\")\n",
    "model = Anime4kUpscale2().to(device).half()\n",
    "dummy_input = torch.randn(1, 3, 720, 1280).to(device).half()\n",
    "onnx_path = \"onnxModel/test.onnx\"\n",
    "torch.onnx.export(\n",
    "        model,\n",
    "        dummy_input,\n",
    "        onnx_path,\n",
    "        input_names=[\"input\"],\n",
    "        output_names=[\"output\"],\n",
    "        dynamic_axes={\n",
    "            \"input\": {0: \"batch_size\", 2: \"height\", 3: \"width\"},\n",
    "            \"output\": {0: \"batch_size\", 2: \"height\", 3: \"width\"},\n",
    "        },\n",
    "        opset_version=11,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(1, 3, 720, 1280).to(device).half()\n",
    "\n",
    "from fvcore.nn import FlopCountAnalysis\n",
    "for filename, name, a, b, c in upscaleModel:\n",
    "    model = Anime4kUpscale(a, b, c).to(device).half()\n",
    "    flops = FlopCountAnalysis(model, dummy_input)\n",
    "    print(name, f': {flops.total() / 10**9:.3f}G')\n",
    "for filename, name, a, b, c in restoreModel:\n",
    "    model = Anime4kRestore(a, b, c).to(device).half()\n",
    "    flops = FlopCountAnalysis(model, dummy_input)\n",
    "    print(name, f': {flops.total() / 10**9:.3f}G')\n",
    "\n",
    "'''\n",
    "720p\n",
    "Restore_Soft_S : 0.829G\n",
    "Restore_Soft_M : 1.847G\n",
    "Restore_Soft_L : 3.782G\n",
    "Restore_Soft_VL : 7.941G\n",
    "Restore_Soft_UL : 17.352G\n",
    "Upscale_L : 3.959G\n",
    "Upscale_VL : 7.852G\n",
    "Upscale_UL : 16.003G\n",
    "\n",
    "GAN_x2_S : 5.187G\n",
    "GAN_x2_M : 9.592G\n",
    "GAN_x3_L : 28.975G\n",
    "GAN_x3_VL : 53.283G\n",
    "GAN_x4_UL : 94.593G\n",
    "GAN_x4_UUL : 220.366G\n",
    "\n",
    "janaiSUC : 41.218G\n",
    "janaiUC : 279.765G\n",
    "janaiC : 551.555G\n",
    "1400.891G\n",
    "\n",
    "1080p\n",
    "Restore_S : 1.866G\n",
    "Restore_Soft_M : 4.155G\n",
    "Restore_Soft_L : 8.510G\n",
    "Restore_Soft_VL : 17.866G\n",
    "Restore_Soft_UL : 39.042G\n",
    "Upscale_L : 8.908G\n",
    "Upscale_VL : 17.667G\n",
    "Upscale_UL : 36.006G\n",
    "janaiSUC : 92.740G\n",
    "janaiUC : 629.470G\n",
    "janaiC : 1241.000G\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fvcore.nn import FlopCountAnalysis\n",
    "\n",
    "dummy_input = torch.randn(1, 3, 1080, 1920).to(device).half()\n",
    "# dummy_input = torch.randn(1, 3, 720, 1280).to(device).half()\n",
    "\n",
    "class compact(nn.Module):\n",
    "    \"\"\"A compact VGG-style network structure for super-resolution.\n",
    "\n",
    "    It is a compact network structure, which performs upsampling in the last layer and no convolution is\n",
    "    conducted on the HR feature space.\n",
    "\n",
    "    Args:\n",
    "        num_in_ch (int): Channel number of inputs. Default: 3.\n",
    "        num_out_ch (int): Channel number of outputs. Default: 3.\n",
    "        num_feat (int): Channel number of intermediate features. Default: 64.\n",
    "        num_conv (int): Number of convolution layers in the body network. Default: 16.\n",
    "        upscale (int): Upsampling factor. Default: 4.\n",
    "        act_type (str): Activation type, options: 'relu', 'prelu', 'leakyrelu'. Default: prelu.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_in_ch=3, num_out_ch=3, num_feat=64, num_conv=16, upscale=2, act_type='prelu', **kwargs):\n",
    "        super(compact, self).__init__()\n",
    "        self.num_in_ch = num_in_ch\n",
    "        self.num_out_ch = num_out_ch\n",
    "        self.num_feat = num_feat\n",
    "        self.num_conv = num_conv\n",
    "        self.upscale = upscale\n",
    "        self.act_type = act_type\n",
    "\n",
    "        self.body = nn.ModuleList()\n",
    "        # the first conv\n",
    "        self.body.append(nn.Conv2d(num_in_ch, num_feat, 3, 1, 1))\n",
    "        # the first activation\n",
    "        if act_type == 'relu':\n",
    "            activation = nn.ReLU(inplace=True)\n",
    "        elif act_type == 'prelu':\n",
    "            activation = nn.PReLU(num_parameters=num_feat)\n",
    "        elif act_type == 'leakyrelu':\n",
    "            activation = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
    "        self.body.append(activation)\n",
    "\n",
    "        # the body structure\n",
    "        for _ in range(num_conv):\n",
    "            self.body.append(nn.Conv2d(num_feat, num_feat, 3, 1, 1))\n",
    "            # activation\n",
    "            if act_type == 'relu':\n",
    "                activation = nn.ReLU(inplace=True)\n",
    "            elif act_type == 'prelu':\n",
    "                activation = nn.PReLU(num_parameters=num_feat)\n",
    "            elif act_type == 'leakyrelu':\n",
    "                activation = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
    "            self.body.append(activation)\n",
    "\n",
    "        # the last conv\n",
    "        self.body.append(nn.Conv2d(num_feat, num_out_ch *\n",
    "                         upscale * upscale, 3, 1, 1))\n",
    "        # upsample\n",
    "        self.upsampler = nn.PixelShuffle(upscale)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for i in range(0, len(self.body)):\n",
    "            out = self.body[i](out)\n",
    "\n",
    "        out = self.upsampler(out)\n",
    "        # add the nearest upsampled image, so that the network learns the residual\n",
    "        base = F.interpolate(x, scale_factor=self.upscale, mode='nearest')\n",
    "        out += base\n",
    "        return out\n",
    "\n",
    "model = compact(num_feat=64, num_conv=16).to(device).half()\n",
    "flops = FlopCountAnalysis(model, dummy_input)\n",
    "print(f': {flops.total() / 10**9:.3f}G')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.0004\n"
     ]
    }
   ],
   "source": [
    "## eval\n",
    "import os\n",
    "import torch\n",
    "from torchvision.transforms import ToTensor\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "# Define a custom dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, low_res_folder, gt_folder, transform=None):\n",
    "        self.low_res_folder = low_res_folder\n",
    "        self.gt_folder = gt_folder\n",
    "        self.transform = transform\n",
    "\n",
    "        self.gt_files = os.listdir(gt_folder)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.gt_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        low_res_path = os.path.join(self.low_res_folder, self.gt_files[idx])\n",
    "        gt_path = os.path.join(self.gt_folder, self.gt_files[idx])\n",
    "\n",
    "        low_res_image = Image.open(low_res_path).convert(\"RGB\")\n",
    "        gt_image = Image.open(gt_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            low_res_image = self.transform(low_res_image)\n",
    "            gt_image = self.transform(gt_image)\n",
    "\n",
    "        return low_res_image.to(device).half(), gt_image.to(device).half()\n",
    "\n",
    "# Create a DataLoader for the dataset\n",
    "transform = ToTensor()  # Convert images to tensors\n",
    "dataset = CustomDataset(\"R:/lr/\", \"R:/hr/\", transform)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "(filename, name, a, b, c) = upscaleModel[0]\n",
    "model = Anime4kUpscale(a, b, c).to(device).half()\n",
    "model.import_param(filename)\n",
    "model.eval()\n",
    "\n",
    "# Define a loss function (e.g., Mean Squared Error)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Initialize a variable to store the total loss\n",
    "total_loss = 0.0\n",
    "\n",
    "# Evaluate the model\n",
    "with torch.no_grad():\n",
    "    for low_res, gt in dataloader:\n",
    "        # Upscale the low-resolution image using the model\n",
    "        upscaled_image = model(low_res)\n",
    "\n",
    "        # Calculate the loss between the upscaled image and ground truth\n",
    "        loss = criterion(upscaled_image, gt)\n",
    "\n",
    "        # Add the loss to the total loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "# Calculate the average loss over the dataset\n",
    "average_loss = total_loss / len(dataset)\n",
    "\n",
    "print(f\"Average Loss: {average_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename, name, a, b, c in upscaleModel:\n",
    "    onnx_path = f\"onnxModel/{name}.onnx\"\n",
    "    model = Anime4kUpscale(a, b, c).to(device).half()\n",
    "    model.import_param(filename)\n",
    "    torch.save(model.state_dict(), f'{name}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32me:\\project\\misc-random-script\\onnx.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/project/misc-random-script/onnx.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m Anime4kUpscale(a, b, c)\u001b[39m.\u001b[39mto(device)\u001b[39m.\u001b[39mhalf()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "model = Anime4kUpscale(a, b, c).to(device).half()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
