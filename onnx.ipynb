{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "import math\n",
    "\n",
    "import torchvision\n",
    "from IPython.display import Image, display\n",
    "from PIL import Image\n",
    "\n",
    "import onnx\n",
    "\n",
    "to_pil = torchvision.transforms.ToPILImage()\n",
    "to_tensor = torchvision.transforms.ToTensor()\n",
    "\n",
    "image = torchvision.io.read_image(\"/Users/khoi.ho/Downloads/110834364_p0.jpg\",torchvision.io.ImageReadMode.RGB) / 256\n",
    "image2 = Image.open(\"/Users/khoi.ho/Downloads/Screenshot 2023-08-21 14-04-25.png\").convert(\"RGB\")\n",
    "image2 = to_tensor(image2)\n",
    "\n",
    "class CReLU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CReLU, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return torch.cat((F.relu(x), F.relu(-x)), 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pass\n",
      "torch.Size([12, 3, 3, 3])\n",
      "0 a\n",
      "torch.Size([12, 24, 3, 3])\n",
      "1 a\n",
      "torch.Size([12, 24, 3, 3])\n",
      "2 a\n",
      "torch.Size([12, 24, 3, 3])\n",
      "3 ----\n",
      "torch.Size([12, 24, 3, 3])\n",
      "4 ----\n",
      "torch.Size([12, 24, 3, 3])\n",
      "5 ----\n",
      "torch.Size([12, 24, 3, 3])\n",
      "6 ----\n",
      "torch.Size([12, 24, 3, 3])\n",
      "7 ----\n",
      "torch.Size([3, 120, 1, 1])\n",
      "out\n"
     ]
    }
   ],
   "source": [
    "class Anime4kRestore(nn.Module):\n",
    "    def __init__(self, block_depth=8, block_stack=5, channel=12):\n",
    "        super(Anime4kRestore, self).__init__()\n",
    "        self.conv_head = nn.Conv2d(3, channel, kernel_size=3, padding=1)\n",
    "        self.conv_mid = nn.ModuleList([nn.Conv2d(\n",
    "            channel*2, channel, kernel_size=3, padding=1) for _ in range(block_depth-1)])\n",
    "        if block_stack != 1:\n",
    "            self.conv_tail = nn.Conv2d(\n",
    "                2*channel*block_stack, 3, kernel_size=1, padding=0)\n",
    "        else:\n",
    "            self.conv_tail = nn.Conv2d(2*channel, 3, kernel_size=3, padding=1)\n",
    "        self.crelu = CReLU()\n",
    "        self.block_no_stack = block_depth - block_stack\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.crelu(self.conv_head(x))\n",
    "        print(self.conv_head.weight.shape)\n",
    "        if self.block_no_stack == 0:\n",
    "            depth_list = [out]\n",
    "            print(0, \"----\")\n",
    "        else:\n",
    "            depth_list = []\n",
    "            print(0, \"a\")\n",
    "        for i, conv in enumerate(self.conv_mid):\n",
    "            out = self.crelu(conv(out))\n",
    "            print(conv.weight.shape)\n",
    "            if i >= self.block_no_stack - 1:\n",
    "                depth_list.append(out)\n",
    "                print(i+1, \"----\")\n",
    "            else:\n",
    "                print(i+1, \"a\")\n",
    "        out = self.conv_tail(torch.cat(depth_list, 1))\n",
    "        print(self.conv_tail.weight.shape)\n",
    "        print(\"out\")\n",
    "        return torch.clamp(out + x, max=1.0, min=0.0)\n",
    "\n",
    "    def import_param(self, filename):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "        with open(filename) as f:\n",
    "            text = f.read()\n",
    "        pattern = r'-?\\d+(\\.\\d{2,})(e-?\\d+)?'\n",
    "        iter = re.finditer(pattern, text)\n",
    "        convert(self.conv_head, iter)\n",
    "        for conv in self.conv_mid:\n",
    "            convert(conv, iter)\n",
    "        convert(self.conv_tail, iter)\n",
    "        check = next(iter, None)\n",
    "        if check == None:\n",
    "            print(\"pass\")\n",
    "        else:\n",
    "            print(\"---failed---\\n\", check)\n",
    "\n",
    "\n",
    "def convert(c, iter):\n",
    "    out_chan, in_chan, width, height = c.weight.shape\n",
    "    # print(c.weight.shape)\n",
    "    for to in range(math.ceil(out_chan/4)):\n",
    "        for ti in range(math.ceil(in_chan/4)):\n",
    "            for w in range(width):\n",
    "                for h in range(height):\n",
    "                    for i in range(min(4, in_chan)):\n",
    "                        for o in range(min(4, out_chan)):\n",
    "                            c.weight.data[to*4+o, ti*4+i, w,\n",
    "                                          h] = float(next(iter).group(0))\n",
    "        for i in range(min(4, out_chan)):\n",
    "            c.bias.data[to*4+i] = float(next(iter).group(0))\n",
    "\n",
    "\n",
    "model = Anime4kRestore(8, 5, 12)\n",
    "model.import_param(\"tmp/Anime4K_Restore_CNN_UL.glsl\")\n",
    "# model = Anime4kRestore(8, 7, 8)\n",
    "# model.import_param(\"tmp/Anime4K_Restore_CNN_VL.glsl\")\n",
    "# model = Anime4kRestore(4, 1, 8)\n",
    "# model.import_param(\"tmp/Anime4K_Restore_CNN_L.glsl\")\n",
    "# model = Anime4kRestore(7, 7, 4)\n",
    "# model.import_param(\"tmp/Anime4K_Restore_CNN_M.glsl\")\n",
    "# model = Anime4kRestore(3, 1, 4)\n",
    "# model.import_param(\"tmp/Anime4K_Restore_CNN_S.glsl\")\n",
    "\n",
    "image2 = Image.open(\n",
    "    \"/Users/khoi.ho/Downloads/Screenshot 2023-08-21 14-04-25.png\").convert(\"RGB\")\n",
    "image2 = to_tensor(image2)\n",
    "# out = model(image2)\n",
    "out = model(image2.unsqueeze(0))[0]\n",
    "# display(to_pil(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Anime4kUpscale(nn.Module):\n",
    "    def __init__(self, block_depth=8, block_stack=5, channel=12):\n",
    "        super(Anime4kUpscale, self).__init__()\n",
    "        self.conv_head = nn.Conv2d(3, channel, kernel_size=3, padding=1)\n",
    "        self.conv_mid = nn.ModuleList([nn.Conv2d(\n",
    "            channel*2, channel, kernel_size=3, padding=1) for _ in range(block_depth-1)])\n",
    "        if block_stack != 1:\n",
    "            self.conv_tail = nn.Conv2d(\n",
    "                2*channel*block_stack, 12, kernel_size=1, padding=0)\n",
    "        else:\n",
    "            self.conv_tail = nn.Conv2d(2*channel, 12, kernel_size=3, padding=1)\n",
    "        self.crelu = CReLU()\n",
    "        self.block_no_stack = block_depth - block_stack\n",
    "        self.ps = nn.PixelShuffle(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.crelu(self.conv_head(x))\n",
    "        print(self.conv_head.weight.shape)\n",
    "        if self.block_no_stack == 0:\n",
    "            depth_list = [out]\n",
    "            print(0, \"----\")\n",
    "        else:\n",
    "            depth_list = []\n",
    "            print(0, \"a\")\n",
    "        for i, conv in enumerate(self.conv_mid):\n",
    "            out = self.crelu(conv(out))\n",
    "            print(conv.weight.shape)\n",
    "            if i >= self.block_no_stack - 1:\n",
    "                depth_list.append(out)\n",
    "                print(i+1, \"----\")\n",
    "            else:\n",
    "                print(i+1, \"a\")\n",
    "        out = self.conv_tail(torch.cat(depth_list, 1))\n",
    "        print(self.conv_tail.weight.shape)\n",
    "        print(\"out\")\n",
    "        out = self.ps(out) + F.interpolate(x, scale_factor=2, mode='bilinear')\n",
    "        return torch.clamp(out, max=1.0, min=0.0)\n",
    "\n",
    "    def import_param(self, filename):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "        with open(filename) as f:\n",
    "            text = f.read()\n",
    "        pattern = r'-?\\d+(\\.\\d{4,})(e-?\\d+)?'\n",
    "        iter = re.finditer(pattern, text)\n",
    "        convert(self.conv_head, iter)\n",
    "        for conv in self.conv_mid:\n",
    "            convert(conv, iter)\n",
    "        convert(self.conv_tail, iter, True)\n",
    "        check = next(iter, None)\n",
    "        if check == None:\n",
    "            print(\"pass\")\n",
    "        else:\n",
    "            print(\"---failed---\\n\", check)\n",
    "        \n",
    "def convert(c, iter, doswap=False):\n",
    "    swap = [0,2,1,3]\n",
    "    out_chan, in_chan, width, height = c.weight.shape\n",
    "    for to in range(math.ceil(out_chan/4)):\n",
    "        for ti in range(math.ceil(in_chan/4)):\n",
    "            for w in range(width):\n",
    "                for h in range(height):\n",
    "                    for i in range(min(4, in_chan)):\n",
    "                        for o in range(min(4, out_chan)):\n",
    "                            o = swap[o] if doswap else o\n",
    "                            c.weight.data[to*4+o, ti*4+i, w, h] = float(next(iter).group(0))\n",
    "        for o in range(min(4, out_chan)):\n",
    "            o = swap[o] if doswap else o\n",
    "            c.bias.data[to*4+o] = float(next(iter).group(0))\n",
    "\n",
    "\n",
    "model = Anime4kUpscale(7, 5, 12)\n",
    "model.import_param(\"tmp/Anime4K_Upscale_CNN_x2_UL.glsl\")\n",
    "# model = Anime4kUpscale(7, 7, 8)\n",
    "# model.import_param(\"tmp/Anime4K_Upscale_CNN_x2_VL.glsl\")\n",
    "# model = Anime4kUpscale(3, 1, 8)\n",
    "# model.import_param(\"tmp/Anime4K_Upscale_CNN_x2_L.glsl\")\n",
    "\n",
    "image2 = Image.open(\n",
    "    \"/Users/khoi.ho/Downloads/Screenshot 2023-08-21 14-04-25.png\").convert(\"RGB\")\n",
    "image2 = to_tensor(image2)\n",
    "out = model(image2.unsqueeze(0))[0]\n",
    "# display(to_pil(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 3, 3, 3])\n",
      "0 a\n",
      "torch.Size([12, 24, 3, 3])\n",
      "1 a\n",
      "torch.Size([12, 24, 3, 3])\n",
      "2 a\n",
      "torch.Size([12, 24, 3, 3])\n",
      "3 ----\n",
      "torch.Size([12, 24, 3, 3])\n",
      "4 ----\n",
      "torch.Size([12, 24, 3, 3])\n",
      "5 ----\n",
      "torch.Size([12, 24, 3, 3])\n",
      "6 ----\n",
      "torch.Size([12, 24, 3, 3])\n",
      "7 ----\n",
      "torch.Size([3, 120, 1, 1])\n",
      "out\n"
     ]
    }
   ],
   "source": [
    "dummy_input = torch.randn(2, 3, 720, 1280)\n",
    "\n",
    "# Export the model to ONNX format\n",
    "onnx_path = \"your_model.onnx\"\n",
    "torch.onnx.export(model, dummy_input, onnx_path)\n",
    "# torch.onnx.export(model, dummy_input, onnx_path, verbose=True)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
