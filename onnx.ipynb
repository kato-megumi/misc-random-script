{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "import math\n",
    "\n",
    "import torchvision\n",
    "from IPython.display import Image, display\n",
    "from PIL import Image\n",
    "\n",
    "# import onnx\n",
    "\n",
    "to_pil = torchvision.transforms.ToPILImage()\n",
    "to_tensor = torchvision.transforms.ToTensor()\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "class CReLU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CReLU, self).__init__()\n",
    "    def forward(self, x):\n",
    "        # return torch.cat((F.relu(x), F.relu(-x)), 1)\n",
    "        return F.relu(torch.cat((x, -x), 1))\n",
    "\n",
    "def convert(c, iter, doswap=False):\n",
    "    swap = [0,2,1,3]\n",
    "    out_chan, in_chan, width, height = c.weight.shape\n",
    "    for to in range(math.ceil(out_chan/4)):\n",
    "        for ti in range(math.ceil(in_chan/4)):\n",
    "            for w in range(width):\n",
    "                for h in range(height):\n",
    "                    for i in range(min(4, in_chan)):\n",
    "                        for o in range(min(4, out_chan)):\n",
    "                            o = swap[o] if doswap else o\n",
    "                            c.weight.data[to*4+o, ti*4+i, w, h] = float(next(iter).group(0))\n",
    "        for o in range(min(4, out_chan)):\n",
    "            o = swap[o] if doswap else o\n",
    "            c.bias.data[to*4+o] = float(next(iter).group(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Anime4kRestore(nn.Module):\n",
    "    def __init__(self, block_depth=8, block_stack=5, channel=12):\n",
    "        super(Anime4kRestore, self).__init__()\n",
    "        self.conv_head = nn.Conv2d(3, channel, kernel_size=3, padding=1)\n",
    "        self.conv_mid = nn.ModuleList([nn.Conv2d(\n",
    "            channel*2, channel, kernel_size=3, padding=1) for _ in range(block_depth-1)])\n",
    "        if block_stack != 1:\n",
    "            self.conv_tail = nn.Conv2d(\n",
    "                2*channel*block_stack, 3, kernel_size=1, padding=0)\n",
    "        else:\n",
    "            self.conv_tail = nn.Conv2d(2*channel, 3, kernel_size=3, padding=1)\n",
    "        self.crelu = CReLU()\n",
    "        self.block_no_stack = block_depth - block_stack\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.crelu(self.conv_head(x))\n",
    "        print(self.conv_head.weight.shape)\n",
    "        if self.block_no_stack == 0:\n",
    "            depth_list = [out]\n",
    "            print(0, \"----\")\n",
    "        else:\n",
    "            depth_list = []\n",
    "            print(0, \"a\")\n",
    "        for i, conv in enumerate(self.conv_mid):\n",
    "            out = self.crelu(conv(out))\n",
    "            print(conv.weight.shape)\n",
    "            if i >= self.block_no_stack - 1:\n",
    "                depth_list.append(out)\n",
    "                print(i+1, \"----\")\n",
    "            else:\n",
    "                print(i+1, \"a\")\n",
    "        out = self.conv_tail(torch.cat(depth_list, 1))\n",
    "        print(self.conv_tail.weight.shape)\n",
    "        print(\"out\")\n",
    "        # return torch.clamp(out + x, max=1.0, min=0.0)\n",
    "        return out + x\n",
    "\n",
    "    def import_param(self, filename):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "        with open(filename) as f:\n",
    "            text = f.read()\n",
    "        pattern = r'-?\\d+(\\.\\d{2,})(e-?\\d+)?'\n",
    "        iter = re.finditer(pattern, text)\n",
    "        convert(self.conv_head, iter)\n",
    "        for conv in self.conv_mid:\n",
    "            convert(conv, iter)\n",
    "        convert(self.conv_tail, iter)\n",
    "        check = next(iter, None)\n",
    "        if check == None:\n",
    "            print(\"pass\")\n",
    "        else:\n",
    "            print(\"---failed---\\n\", check)\n",
    "\n",
    "# model = Anime4kRestore(8, 5, 12).to(device)\n",
    "# model.import_param(\"tmp/Anime4K_Restore_CNN_UL.glsl\")\n",
    "# model = Anime4kRestore(8, 7, 8)\n",
    "# model.import_param(\"tmp/Anime4K_Restore_CNN_VL.glsl\")\n",
    "# model = Anime4kRestore(4, 1, 8)\n",
    "# model.import_param(\"tmp/Anime4K_Restore_CNN_L.glsl\")\n",
    "# model = Anime4kRestore(7, 7, 4)\n",
    "# model.import_param(\"tmp/Anime4K_Restore_CNN_M.glsl\")\n",
    "# model = Anime4kRestore(3, 1, 4)\n",
    "# model.import_param(\"tmp/Anime4K_Restore_CNN_S.glsl\")\n",
    "\n",
    "# image2 = Image.open(\n",
    "#     \"C://Users/khoi/Videos/Screenshot 2023-08-21 14-04-25.png\").convert(\"RGB\")\n",
    "# image2 = to_tensor(image2).unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "# out = model(image2)[0]\n",
    "# display(to_pil(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Anime4kUpscale(nn.Module):\n",
    "    def __init__(self, block_depth=8, block_stack=5, channel=12):\n",
    "        super(Anime4kUpscale, self).__init__()\n",
    "        self.conv_head = nn.Conv2d(3, channel, kernel_size=3, padding=1)\n",
    "        self.conv_mid = nn.ModuleList([nn.Conv2d(\n",
    "            channel*2, channel, kernel_size=3, padding=1) for _ in range(block_depth-1)])\n",
    "        if block_stack != 1:\n",
    "            self.conv_tail = nn.Conv2d(\n",
    "                2*channel*block_stack, 12, kernel_size=1, padding=0)\n",
    "        else:\n",
    "            self.conv_tail = nn.Conv2d(2*channel, 12, kernel_size=3, padding=1)\n",
    "        self.crelu = CReLU()\n",
    "        self.block_no_stack = block_depth - block_stack\n",
    "        self.ps = nn.PixelShuffle(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.crelu(self.conv_head(x))\n",
    "        print(self.conv_head.weight.shape)\n",
    "        if self.block_no_stack == 0:\n",
    "            depth_list = [out]\n",
    "            print(0, \"----\")\n",
    "        else:\n",
    "            depth_list = []\n",
    "            print(0, \"a\")\n",
    "        for i, conv in enumerate(self.conv_mid):\n",
    "            out = self.crelu(conv(out))\n",
    "            print(conv.weight.shape)\n",
    "            if i >= self.block_no_stack - 1:\n",
    "                depth_list.append(out)\n",
    "                print(i+1, \"----\")\n",
    "            else:\n",
    "                print(i+1, \"a\")\n",
    "        out = self.conv_tail(torch.cat(depth_list, 1))\n",
    "        print(self.conv_tail.weight.shape)\n",
    "        print(\"out\")\n",
    "        out = self.ps(out) + F.interpolate(x, scale_factor=2, mode='bilinear')\n",
    "        # return torch.clamp(out, max=1.0, min=0.0)\n",
    "        return out\n",
    "\n",
    "    def import_param(self, filename):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "        with open(filename) as f:\n",
    "            text = f.read()\n",
    "        pattern = r'-?\\d+(\\.\\d{4,})(e-?\\d+)?'\n",
    "        iter = re.finditer(pattern, text)\n",
    "        convert(self.conv_head, iter)\n",
    "        for conv in self.conv_mid:\n",
    "            convert(conv, iter)\n",
    "        convert(self.conv_tail, iter, True)\n",
    "        check = next(iter, None)\n",
    "        if check == None:\n",
    "            print(\"pass\")\n",
    "        else:\n",
    "            print(\"---failed---\\n\", check)\n",
    "\n",
    "\n",
    "# model = Anime4kUpscale(7, 5, 12).to(device)\n",
    "# model.import_param(\"tmp/Anime4K_Upscale_CNN_x2_UL.glsl\")\n",
    "# model = Anime4kUpscale(7, 7, 8)\n",
    "# model.import_param(\"tmp/Anime4K_Upscale_CNN_x2_VL.glsl\")\n",
    "# model = Anime4kUpscale(3, 1, 8)\n",
    "# model.import_param(\"tmp/Anime4K_Upscale_CNN_x2_L.glsl\")\n",
    "\n",
    "# image2 = Image.open(\n",
    "#     \"C://Users/khoi/Videos/Screenshot 2023-08-21 14-04-25.png\").convert(\"RGB\")\n",
    "# image2 = to_tensor(image2).unsqueeze(0).to(device)\n",
    "# out = model(image2)[0]\n",
    "# display(to_pil(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.0+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "============= Diagnostic Run torch.onnx.export version 2.0.0+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "============= Diagnostic Run torch.onnx.export version 2.0.0+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "============= Diagnostic Run torch.onnx.export version 2.0.0+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "============= Diagnostic Run torch.onnx.export version 2.0.0+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "============= Diagnostic Run torch.onnx.export version 2.0.0+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "============= Diagnostic Run torch.onnx.export version 2.0.0+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "============= Diagnostic Run torch.onnx.export version 2.0.0+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "============= Diagnostic Run torch.onnx.export version 2.0.0+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "============= Diagnostic Run torch.onnx.export version 2.0.0+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "============= Diagnostic Run torch.onnx.export version 2.0.0+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "============= Diagnostic Run torch.onnx.export version 2.0.0+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "============= Diagnostic Run torch.onnx.export version 2.0.0+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "============= Diagnostic Run torch.onnx.export version 2.0.0+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "============= Diagnostic Run torch.onnx.export version 2.0.0+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "============= Diagnostic Run torch.onnx.export version 2.0.0+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dummy_input = torch.randn(1, 3, 720, 1280).to(device).half()\n",
    "\n",
    "upscaleModel = [\n",
    "    (\"tmp/Anime4K_Upscale_CNN_x2_L.glsl\", \"Upscale_L\", 3, 1, 8),\n",
    "    (\"tmp/Anime4K_Upscale_CNN_x2_VL.glsl\", \"Upscale_VL\", 7, 7, 8),\n",
    "    (\"tmp/Anime4K_Upscale_CNN_x2_UL.glsl\", \"Upscale_UL\", 7, 5, 12),\n",
    "    (\"tmp/Anime4K_Upscale_Denoise_CNN_x2_L.glsl\", \"Upscale_Denoise_L\", 3, 1, 8),\n",
    "    (\"tmp/Anime4K_Upscale_Denoise_CNN_x2_VL.glsl\", \"Upscale_Denoise_VL\", 7, 7, 8),\n",
    "    (\"tmp/Anime4K_Upscale_Denoise_CNN_x2_UL.glsl\", \"Upscale_Denoise_UL\", 7, 5, 12),\n",
    "]\n",
    "restoreModel = [\n",
    "    (\"tmp/Anime4K_Restore_CNN_S.glsl\", \"Restore_S\", 3, 1, 4),\n",
    "    (\"tmp/Anime4K_Restore_CNN_M.glsl\", \"Restore_M\", 7, 7, 4),\n",
    "    (\"tmp/Anime4K_Restore_CNN_L.glsl\", \"Restore_L\", 4, 1, 8),\n",
    "    (\"tmp/Anime4K_Restore_CNN_VL.glsl\", \"Restore_VL\", 8, 7, 8),\n",
    "    (\"tmp/Anime4K_Restore_CNN_UL.glsl\", \"Restore_UL\", 8, 5, 12),\n",
    "    (\"tmp/Anime4K_Restore_CNN_Soft_S.glsl\", \"Restore_S\", 3, 1, 4),\n",
    "    (\"tmp/Anime4K_Restore_CNN_Soft_M.glsl\", \"Restore_Soft_M\", 7, 7, 4),\n",
    "    (\"tmp/Anime4K_Restore_CNN_Soft_L.glsl\", \"Restore_Soft_L\", 4, 1, 8),\n",
    "    (\"tmp/Anime4K_Restore_CNN_Soft_VL.glsl\", \"Restore_Soft_VL\", 8, 7, 8),\n",
    "    (\"tmp/Anime4K_Restore_CNN_Soft_UL.glsl\", \"Restore_Soft_UL\", 8, 5, 12),\n",
    "]\n",
    "\n",
    "\n",
    "def print(*args):\n",
    "    pass\n",
    "\n",
    "\n",
    "for filename, name, a, b, c in upscaleModel:\n",
    "    onnx_path = f\"onnxModel/{name}.onnx\"\n",
    "    model = Anime4kUpscale(a, b, c).to(device).half()\n",
    "    model.import_param(filename)\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        dummy_input,\n",
    "        onnx_path,\n",
    "        input_names=[\"input\"],\n",
    "        output_names=[\"output\"],\n",
    "        dynamic_axes={\n",
    "            \"input\": {0: \"batch_size\", 2: \"width\", 3: \"height\"},\n",
    "            \"output\": {0: \"batch_size\", 2: \"width\", 3: \"height\"},\n",
    "        },\n",
    "    )\n",
    "    # torch.onnx.export(model, dummy_input, onnx_path, verbose=True)\n",
    "\n",
    "for filename, name, a, b, c in restoreModel:\n",
    "    onnx_path = f\"onnxModel/{name}.onnx\"\n",
    "    model = Anime4kRestore(a, b, c).to(device).half()\n",
    "    model.import_param(filename)\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        dummy_input,\n",
    "        onnx_path,\n",
    "        input_names=[\"input\"],\n",
    "        output_names=[\"output\"],\n",
    "        dynamic_axes={\n",
    "            \"input\": {0: \"batch_size\", 2: \"height\", 3: \"width\"},\n",
    "            \"output\": {0: \"batch_size\", 2: \"height\", 3: \"width\"},\n",
    "        },\n",
    "        opset_version=10,\n",
    "    )\n",
    "    # torch.onnx.export(model, dummy_input, onnx_path, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1792037378.py, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 16\u001b[1;36m\u001b[0m\n\u001b[1;33m    self.prelu = nn.ModuleList[nn.PReLU() for i in range(channel)]\u001b[0m\n\u001b[1;37m                                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Anime4kUpscale2(nn.Module):\n",
    "    def __init__(self, block_depth=7, block_stack=5, channel=12):\n",
    "        super(Anime4kUpscale2, self).__init__()\n",
    "        self.conv_head = nn.Conv2d(3, channel, kernel_size=3, padding=1)\n",
    "        self.conv_mid = nn.ModuleList([nn.Conv2d(\n",
    "            channel, channel, kernel_size=3, padding=1) for _ in range(block_depth-1)])\n",
    "        if block_stack != 1:\n",
    "            self.conv_tail = nn.Conv2d(\n",
    "                channel*block_stack, 12, kernel_size=1, padding=0)\n",
    "        else:\n",
    "            self.conv_tail = nn.Conv2d(channel, 12, kernel_size=3, padding=1)\n",
    "        self.prelu = nn.ModuleList[nn.PReLU() for i in range(block_depth)]\n",
    "        self.block_no_stack = block_depth - block_stack\n",
    "        self.ps = nn.PixelShuffle(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.prelu[0](self.conv_head(x))\n",
    "        print(self.conv_head.weight.shape)\n",
    "        if self.block_no_stack == 0:\n",
    "            depth_list = [out]\n",
    "        else:\n",
    "            depth_list = []\n",
    "        for i, conv in enumerate(self.conv_mid):\n",
    "            out = self.prelu[i+1](conv(out))\n",
    "            print(conv.weight.shape)\n",
    "            if i >= self.block_no_stack - 1:\n",
    "                depth_list.append(out)\n",
    "        out = self.conv_tail(torch.cat(depth_list, 1))\n",
    "        out = self.ps(out) + F.interpolate(x, scale_factor=2, mode='bilinear')\n",
    "        # return torch.clamp(out, max=1.0, min=0.0)\n",
    "        return out\n",
    "    \n",
    "device = torch.device(\"cuda\")\n",
    "    \n",
    "model = Anime4kUpscale2().to(device).half()\n",
    "dummy_input = torch.randn(1, 3, 720, 1280).to(device).half()\n",
    "onnx_path = \"onnxModel/test.onnx\"\n",
    "torch.onnx.export(\n",
    "        model,\n",
    "        dummy_input,\n",
    "        onnx_path,\n",
    "        input_names=[\"input\"],\n",
    "        output_names=[\"output\"],\n",
    "        dynamic_axes={\n",
    "            \"input\": {0: \"batch_size\", 2: \"height\", 3: \"width\"},\n",
    "            \"output\": {0: \"batch_size\", 2: \"height\", 3: \"width\"},\n",
    "        },\n",
    "        opset_version=11,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
