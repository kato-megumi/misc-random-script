{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "\n",
    "def make_layer(basic_block, num_basic_block, **kwarg):\n",
    "    \"\"\"Make layers by stacking the same blocks.\n",
    "\n",
    "    Args:\n",
    "        basic_block (nn.module): nn.module class for basic block.\n",
    "        num_basic_block (int): number of blocks.\n",
    "\n",
    "    Returns:\n",
    "        nn.Sequential: Stacked blocks in nn.Sequential.\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    for _ in range(num_basic_block):\n",
    "        layers.append(basic_block(**kwarg))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class MDBN(nn.Module):\n",
    "    def __init__(self, num_in_ch=3, num_out_ch=3, num_feat=64, num_block=6, upscale=2, res_scale=1.0):\n",
    "        super(MDBN, self).__init__()\n",
    "        self.conv_first = nn.Conv2d(num_in_ch, num_feat, 3, 1, 1)\n",
    "        self.body = make_layer(ResidualBlock, num_block, num_feat=num_feat, res_scale=res_scale)\n",
    "\n",
    "        self.conv_after_body = nn.Conv2d(num_feat, num_feat, 3, 1, 1)\n",
    "\n",
    "        self.upsample = Upsample(upscale, num_feat)\n",
    "\n",
    "        self.conv_last = nn.Conv2d(num_feat, num_out_ch, 3, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_first(x)\n",
    "        res = self.conv_after_body(self.body(x))\n",
    "        res += x\n",
    "        x = self.conv_last(self.upsample(res))\n",
    "\n",
    "        return x\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, num_feat=64, res_scale=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.res_scale = res_scale\n",
    "        self.baseblock1 = BaseBlock(num_feat)\n",
    "        self.baseblock2 = BaseBlock(num_feat)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        x = self.baseblock1(x)\n",
    "        x = self.baseblock2(x)\n",
    "\n",
    "        return identity + x * self.res_scale\n",
    "\n",
    "class BaseBlock(nn.Module):\n",
    "    def __init__(self, num_feat):\n",
    "        super(BaseBlock, self).__init__()\n",
    "        self.uconv1 = nn.Conv2d(num_feat, num_feat, 3, 1, 1)\n",
    "        self.uconv2 = nn.Conv2d(num_feat, num_feat, 3, 1, 1)\n",
    "        self.dconv = nn.Conv2d(num_feat, num_feat, 3, 1, 1)\n",
    "        self.act = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.uconv2(self.act(self.uconv1(x)))\n",
    "        x2 = self.dconv(x)\n",
    "        x = self.act(x1 + x2)\n",
    "        return x\n",
    "\n",
    "class Upsample(nn.Sequential):\n",
    "    \"\"\"Upsample module.\n",
    "\n",
    "    Args:\n",
    "        scale (int): Scale factor. Supported scales: 2^n and 3.\n",
    "        num_feat (int): Channel number of intermediate features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scale, num_feat):\n",
    "        m = []\n",
    "        if (scale & (scale - 1)) == 0:  # scale = 2^n\n",
    "            for _ in range(int(math.log(scale, 2))):\n",
    "                m.append(nn.Conv2d(num_feat, 4 * num_feat, 3, 1, 1))\n",
    "                m.append(nn.PixelShuffle(2))\n",
    "        elif scale == 3:\n",
    "            m.append(nn.Conv2d(num_feat, 9 * num_feat, 3, 1, 1))\n",
    "            m.append(nn.PixelShuffle(3))\n",
    "        else:\n",
    "            raise ValueError(f'scale {scale} is not supported. Supported scales: 2^n and 3.')\n",
    "        super(Upsample, self).__init__(*m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "dummy_input = torch.randn(1, 3, 256, 256).to(device)\n",
    "model = MDBN(upscale=4).to(device)\n",
    "model.load_state_dict(torch.load(\"R:/MDBN_x4.pth\", map_location=device)['params'])\n",
    "\n",
    "# from fvcore.nn import FlopCountAnalysis\n",
    "# dummy_input = torch.randn(1, 3, 720, 1280).to(device)\n",
    "# flops = FlopCountAnalysis(model, dummy_input)\n",
    "# print(f': {flops.total() / 10**9:.3f}G')\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    \"R:/MDBN_x4.onnx\",\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\n",
    "        \"input\": {0: \"batch_size\", 2: \"height\", 3: \"width\"},\n",
    "        \"output\": {0: \"batch_size\", 2: \"height\", 3: \"width\"},\n",
    "    },\n",
    "    opset_version=11,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
